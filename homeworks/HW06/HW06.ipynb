{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def print_metrics(y_true, y_pred, y_proba=None, title=None):\n",
    "        # y_true, y_pred: истинные и предсказанные метки классов (например, 0/1)\n",
    "        # y_proba: вероятность класса 1 (нужна, если хотим посчитать ROC-AUC)\n",
    "        if title:\n",
    "            print(title)\n",
    "        # Accuracy – доля правильных ответов (хороша как “первая” метрика, но не всегда достаточно)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        # F1 – баланс precision/recall (часто полезнее accuracy при дисбалансе классов)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        print(f\"accuracy = {acc:.4f}\")\n",
    "        print(f\"f1       = {f1:.4f}\")\n",
    "        if y_proba is not None:\n",
    "            # ROC-AUC корректно считать по вероятностям/скорингам, а не по “жёстким” классам\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true, y_proba)\n",
    "                print(f\"roc_auc  = {auc:.4f}\")\n",
    "            except Exception:\n",
    "                # Иногда AUC может не считаться (например, если в y_true один класс)\n",
    "                pass\n",
    "        print(\"confusion_matrix:\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mirea-aie-2025/aie-course-meta/refs/heads/main/seminars/S06/S06-hw-dataset-01.csv'\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())\n",
    "print('=' * 52)\n",
    "print(df.info())\n",
    "print('=' * 52)\n",
    "print(df.describe())\n",
    "print('=' * 52)\n",
    "X = df.drop('id', axis=1)\n",
    "X = X.drop('target', axis=1)\n",
    "y = df.target\n",
    "print('Доля классов:', abs(1 -len(df[df.target == 1])/len(df)), ':', len(df[df.target == 1])/len(df))\n",
    "print('=' * 52)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "bl = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "bl.fit(X_train, y_train)\n",
    "y_proba = bl.predict_proba(X_test)[:, 1]\n",
    "y_pred = bl.predict(X_test)\n",
    "print_metrics(y_test, y_pred, y_proba=y_proba, title='Dummy')\n",
    "print('=' * 52)\n",
    "\n",
    "lr = Pipeline(steps=[('skelter', StandardScaler()), (\"logreg\", LogisticRegression(penalty='l2' , C=1.0, solver='liblinear', random_state=42))])\n",
    "lr.fit(X_train, y_train)\n",
    "y_lr_pred = lr.predict(X_test)\n",
    "y_lr_proba = lr.predict_proba(X_test)[:, 1]\n",
    "print_metrics(y_test, y_lr_pred, y_proba=y_lr_proba, title='LogisticRegression')\n",
    "print('=' * 52)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_leaf=10)\n",
    "dt.fit(X_train, y_train)\n",
    "y_dt_proba = dt.predict_proba(X_test)[:, 1]\n",
    "y_dt_pred = (y_dt_proba >= 0.5).astype(int)\n",
    "print_metrics(y_test, y_dt_pred, y_proba=y_dt_proba, title='DecisionTree')\n",
    "print('=' * 52)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True, n_estimators=600, max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_rf_proba = rf.predict_proba(X_test)[:, 1]\n",
    "y_rf_pred = (y_rf_proba >= 0.5).astype(int)\n",
    "print_metrics(y_test, y_rf_pred, y_proba=y_rf_proba, title='RandomForest')\n",
    "print('=' * 52)\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42, estimator=dt, n_estimators=200, learning_rate=0.6)\n",
    "ada.fit(X_train, y_train)\n",
    "pred = ada.predict(X_test)\n",
    "proba = None\n",
    "if hasattr(ada, \"predict_proba\"):\n",
    "    proba = ada.predict_proba(X_test)[:, 1]\n",
    "print_metrics(y_test, pred, y_proba=proba, title='AdaBoost')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
